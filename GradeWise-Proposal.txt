Project Proposal: GradeWise
Project Title: GradeWise – Intelligent Agentic Grading & Feedback System
1. Executive Overview
GradeWise is an enterprise-grade agentic AI application designed to revolutionize the educational assessment process. It addresses the critical bottleneck in education: the time-consuming nature of manual grading and the lack of immediate, personalized feedback.
Unlike simple automated graders, GradeWise utilizes a Retrieval-Augmented Generation (RAG) architecture. It acts as an intelligent teaching assistant that ingests course materials (textbooks, lecture notes) to understand the subject context and applies a specific rubric given by the educator to grade assessments.
To ensure a high-quality user experience, the application features a decoupled architecture: a beautiful, responsive frontend designed with AI-assisted tools (Google Stitch) and a robust, scalable backend API.
2. Problem Statement
	•	Educator Burnout: Teachers spend hundreds of hours grading routine assignments, reducing time for mentorship.
	•	Feedback Latency: Students often receive feedback weeks after submission, missing the critical window for learning.
	•	Grading Inconsistency: Subjective grading can vary between different graders (TAs) or even the same grader over time.
3. Major Functions
The system operates as an Agentic Workflow, exposed via a RESTful API.
A. Intelligent Ingestion (API)
	•	Dynamic Rubrics: Accepts PDF/DOCX rubrics and parses them into structured JSON criteria.
	•	Context Learning: Ingests course materials (PDF textbooks, slides) and indexes them into a Vector Database for fact-checking.
B. The Grading Agent (Logic)
	•	Reasoning Engine: Uses Chain-of-Thought (CoT) prompting to analyze student submissions section by section.
	•	RAG Verification: Cross-references student claims against the uploaded course material to prevent "hallucinations."
	•	Rubric Alignment: Strict scoring based only on the provided rubric criteria.
C. Professional Dashboard (UI)
	•	Split-Screen View: A modern React-based interface allowing the educator to see the Student Essay on the left and the AI’s real-time analysis on the right.
	•	Interactive Feedback: Educators can click to "Approve" grades or "Edit" feedback.
	•	Batch Management: A dashboard to manage multiple students and export grades.
4. Technical Architecture (Modern & Scalable)
The project uses a Decoupled Architecture, separating the "Look" (Frontend) from the "Brain" (Backend).
A. Frontend (The Interface)
	•	Framework: React.js (via Next.js) + Tailwind CSS.
	•	Design Source: Google Stitch / AI Generators. (Leveraging AI tools to create production-ready UI components).
B. Backend (The Brain)
	•	API Framework: FastAPI (Python).
	•	Orchestration: LangGraph (Manages the complex looping logic of the agent).
C. AI & Data Layer (Cost-Optimized)
	•	LLM: Llama 3 (70B) via Groq or Gemini 1.5 Pro.
	•	Vector Database: ChromaDB (Running locally/Dockerized to avoid cloud costs).
5. Data Strategy
A. Dynamic Context
The system uses RAG (Retrieval-Augmented Generation) rather than fine-tuning.
	•	Input: Educator uploads files via the UI.
	•	Process: The backend chunks these files and stores embeddings in ChromaDB.
B. Benchmarking Datasets (Validation)
To evaluate the system before deployment, public datasets will be used:
	•	The Hewlett Foundation (ASAP) Dataset: A Kaggle dataset containing thousands of student essays with human-graded scores.
	•	Usage: We will run GradeWise on a subset of these essays and compare the AI's score against the human ground truth to prove reliability.
	•	Synthetic Data: We will generate synthetic "Student Submissions" using a high-end model (like GPT-4o) to create specific edge cases (e.g., an essay that is grammatically perfect but factually wrong) to ensure the agent catches them.
6. Evaluation & Improvement Plan
To ensure the system is reliable, a robust evaluation framework ("Grading the Grader") will be implemented.
Step 1: The Golden Set (Ground Truth)
	•	We will collect a sample of 20–30 diverse assignments.
	•	A human subject matter expert will grade these manually.
	•	These human grades serve as the "Ground Truth."
Step 2: Automated Metrics
	•	Score Correlation: We will calculate the statistical correlation between the AI's scores and the Human's scores. A high correlation (>0.8) indicates reliability.
	•	Feedback Relevance: We will use a "Judge LLM" (a separate instance of the LLM) to read the feedback generated by GradeWise and rate it on criteria such as Tone, Specificity, and Accuracy.
Step 3: Iterative Improvement
	•	Prompt Engineering: If the system is consistently "too harsh," we will refine the System Prompt.
	•	Few-Shot Prompting: We will feed examples of "Good Feedback" vs. "Bad Feedback" into the prompt context so the model learns the desired style.
7. Privacy & Operational Costs
A. Data Privacy
GradeWise is designed with a "Privacy-First" approach.
	•	Local Storage: Student essays and grades are stored in a local SQLite database, ensuring sensitive data does not leave the school's control.
	•	Transient Processing: Data sent to the LLM is for processing only and is not used to train public models (via enterprise API agreements).
B. Estimated Costs
The project is architected for maximum cost-efficiency:
	•	Development: $0 (Open Source Libraries).
	•	LLM Inference: ~$0 - $5/month (Using Groq Free Tier or Gemini Flash).
	•	Hosting: Self-hosted (Free) or minimal cloud instance (~$10/month).
8. Project Roadmap
	•	Phase 1: Design & Frontend: Generate UI components using Google Stitch and assemble the React App.
	•	Phase 2: The API Engine: Build the FastAPI backend and Agentic Logic (LangGraph + Llama 3).
	•	Phase 3: Integration: Connect the React Frontend to the FastAPI Backend.
	•	Phase 4: Testing & Deployment: Run the "Golden Set" evaluation using the Hewlett Foundation dataset and Dockerize the application.

